{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqtoSeq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYlNw4r_dCKd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsDKDZdyZyzm"
      },
      "source": [
        "Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y__kYfoqdFr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a8e34a-a19b-49e7-f6d4-bc3249cfe08d"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfg9_eY1cqGy"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xuoJ34ZZrS-"
      },
      "source": [
        "# Convert unicode to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVIiVssda4Zc"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # Creating a space between a word and the punctuation following it\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # Add start and end token to sentence\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YAYoUCSa5iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2760018b-54f5-4867-836b-579d721a7128"
      },
      "source": [
        "# Example\n",
        "sentence = \"How is the weather today?\"\n",
        "print(preprocess_sentence(sentence))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> how is the weather today ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTPeVjhndVNu"
      },
      "source": [
        "Proprocess and create the datset in the format :[Engilsh, Spanish]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJNR0mIbdDMs"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxLBJDB9eJbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5ee85f-a94e-463c-b786-95950e35e75f"
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[101])\n",
        "print(sp[101])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> go away ! <end>\n",
            "<start> largo ! <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puuio_sMeZzO"
      },
      "source": [
        "def tokenize(lang):\n",
        "\n",
        "  # Convert text corpus into vector, converting the words into integers. Each integer is the index of the word in the vocabulary dictionary.\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "\n",
        "  # Updates the internal vocabulary dictionary with the given text.\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  # Transforms text into sequence of integers\n",
        "  # Returns python list/array of sequences\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  # Returns numpy array of sequences with padding upto the maximum length\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4cMv2yi9edL"
      },
      "source": [
        "https://medium.com/analytics-vidhya/tutorial-on-bucket-by-sequence-length-api-for-efficiently-batching-nlp-data-while-training-20d8ef5219d7\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJvksbDofl7d"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  inp_lang, targ_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ean4j0-LDA_"
      },
      "source": [
        "Limit the size of Dataset for faster experimenting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERiiqycOfohy"
      },
      "source": [
        "size = 30000\n",
        "# input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, size)\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K2j6ybA6wfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef242912-c9dc-4a90-8f25-1ec3d432d867"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95171 95171 23793 23793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb1RlmgEsTaF"
      },
      "source": [
        "Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Op-eFogXN1s"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "units = 1024\n",
        "d_model = 256\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXw5LL9jJ72z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51011a0-0dcb-44e0-924f-6658d5cf83fb"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 51]), TensorShape([128, 53]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYOu7GCeVnVh"
      },
      "source": [
        "Model: Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwhEXJkirGYZ"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, d_model, n_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.n_units = n_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.gru = tf.keras.layers.GRU(self.n_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.n_units))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5tqABnoQTkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad88be46-d49f-4cfc-f31c-8e5921bba07e"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, d_model, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 51, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LBMQaRIVsjz"
      },
      "source": [
        "Model: Attention Layer|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiE4Pw7BLDPI"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlbj4O--MY_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b90abd-3354-4731-c8b1-75456764d350"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (128, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 51, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElTR6KlWVw71"
      },
      "source": [
        "Model: Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smtkYS59JY6o"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, d_model, dec_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, d_model)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, d_model + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc5WCxijK3Eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586dc859-f13a-40b6-a1da-d49df42be9d6"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, d_model, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 24794)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsmVxgkRMv1Q"
      },
      "source": [
        "Oprtimizer and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjoJXjUyMzIe"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVwbc1y0MziM"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAJ78_-eP4J2"
      },
      "source": [
        "Creating checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjP0vxBhPu5Q"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNPdrrI42IOk"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpvELQ32HhL"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpjGiLlUP86W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca669a1-503d-4d18-c034-cf046e294ee7"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.5899\n",
            "Epoch 1 Batch 100 Loss 0.9185\n",
            "Epoch 1 Batch 200 Loss 0.7715\n",
            "Epoch 1 Batch 300 Loss 0.7749\n",
            "Epoch 1 Batch 400 Loss 0.7390\n",
            "Epoch 1 Batch 500 Loss 0.6689\n",
            "Epoch 1 Batch 600 Loss 0.6622\n",
            "Epoch 1 Batch 700 Loss 0.6311\n",
            "Epoch 1 Loss 0.7604\n",
            "Time taken for 1 epoch 610.1199007034302 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6575\n",
            "Epoch 2 Batch 100 Loss 0.6184\n",
            "Epoch 2 Batch 200 Loss 0.5951\n",
            "Epoch 2 Batch 300 Loss 0.5666\n",
            "Epoch 2 Batch 400 Loss 0.5668\n",
            "Epoch 2 Batch 500 Loss 0.5336\n",
            "Epoch 2 Batch 600 Loss 0.4917\n",
            "Epoch 2 Batch 700 Loss 0.4594\n",
            "Epoch 2 Loss 0.5416\n",
            "Time taken for 1 epoch 561.7403774261475 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4734\n",
            "Epoch 3 Batch 100 Loss 0.4056\n",
            "Epoch 3 Batch 200 Loss 0.4032\n",
            "Epoch 3 Batch 300 Loss 0.3634\n",
            "Epoch 3 Batch 400 Loss 0.3812\n",
            "Epoch 3 Batch 500 Loss 0.3985\n",
            "Epoch 3 Batch 600 Loss 0.3845\n",
            "Epoch 3 Batch 700 Loss 0.3019\n",
            "Epoch 3 Loss 0.3801\n",
            "Time taken for 1 epoch 560.0111970901489 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2838\n",
            "Epoch 4 Batch 100 Loss 0.2601\n",
            "Epoch 4 Batch 200 Loss 0.2606\n",
            "Epoch 4 Batch 300 Loss 0.2925\n",
            "Epoch 4 Batch 400 Loss 0.2802\n",
            "Epoch 4 Batch 500 Loss 0.2401\n",
            "Epoch 4 Batch 600 Loss 0.2268\n",
            "Epoch 4 Batch 700 Loss 0.2520\n",
            "Epoch 4 Loss 0.2609\n",
            "Time taken for 1 epoch 561.520834684372 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1972\n",
            "Epoch 5 Batch 100 Loss 0.2231\n",
            "Epoch 5 Batch 200 Loss 0.1844\n",
            "Epoch 5 Batch 300 Loss 0.1918\n",
            "Epoch 5 Batch 400 Loss 0.2314\n",
            "Epoch 5 Batch 500 Loss 0.2253\n",
            "Epoch 5 Batch 600 Loss 0.1839\n",
            "Epoch 5 Batch 700 Loss 0.1852\n",
            "Epoch 5 Loss 0.1921\n",
            "Time taken for 1 epoch 560.0742619037628 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1481\n",
            "Epoch 6 Batch 100 Loss 0.1310\n",
            "Epoch 6 Batch 200 Loss 0.1476\n",
            "Epoch 6 Batch 300 Loss 0.1383\n",
            "Epoch 6 Batch 400 Loss 0.1691\n",
            "Epoch 6 Batch 500 Loss 0.1769\n",
            "Epoch 6 Batch 600 Loss 0.1506\n",
            "Epoch 6 Batch 700 Loss 0.1559\n",
            "Epoch 6 Loss 0.1504\n",
            "Time taken for 1 epoch 561.7710666656494 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1407\n",
            "Epoch 7 Batch 100 Loss 0.1149\n",
            "Epoch 7 Batch 200 Loss 0.1268\n",
            "Epoch 7 Batch 300 Loss 0.1122\n",
            "Epoch 7 Batch 400 Loss 0.1316\n",
            "Epoch 7 Batch 500 Loss 0.1468\n",
            "Epoch 7 Batch 600 Loss 0.1250\n",
            "Epoch 7 Batch 700 Loss 0.1319\n",
            "Epoch 7 Loss 0.1228\n",
            "Time taken for 1 epoch 560.1273331642151 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0822\n",
            "Epoch 8 Batch 100 Loss 0.1047\n",
            "Epoch 8 Batch 200 Loss 0.0847\n",
            "Epoch 8 Batch 300 Loss 0.0805\n",
            "Epoch 8 Batch 400 Loss 0.1078\n",
            "Epoch 8 Batch 500 Loss 0.1046\n",
            "Epoch 8 Batch 600 Loss 0.0857\n",
            "Epoch 8 Batch 700 Loss 0.1015\n",
            "Epoch 8 Loss 0.0987\n",
            "Time taken for 1 epoch 561.4501485824585 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0755\n",
            "Epoch 9 Batch 100 Loss 0.0716\n",
            "Epoch 9 Batch 200 Loss 0.0784\n",
            "Epoch 9 Batch 300 Loss 0.0820\n",
            "Epoch 9 Batch 400 Loss 0.0714\n",
            "Epoch 9 Batch 500 Loss 0.0965\n",
            "Epoch 9 Batch 600 Loss 0.0823\n",
            "Epoch 9 Batch 700 Loss 0.0814\n",
            "Epoch 9 Loss 0.0852\n",
            "Time taken for 1 epoch 559.7291450500488 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0541\n",
            "Epoch 10 Batch 100 Loss 0.0686\n",
            "Epoch 10 Batch 200 Loss 0.0649\n",
            "Epoch 10 Batch 300 Loss 0.0594\n",
            "Epoch 10 Batch 400 Loss 0.0711\n",
            "Epoch 10 Batch 500 Loss 0.0682\n",
            "Epoch 10 Batch 600 Loss 0.0687\n",
            "Epoch 10 Batch 700 Loss 0.0695\n",
            "Epoch 10 Loss 0.0726\n",
            "Time taken for 1 epoch 561.1947598457336 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aStxBTGI3hzG"
      },
      "source": [
        "np.save('inp_lang_word_index.npy', inp_lang.word_index)\n",
        "np.save('inp_lang_index_word.npy', inp_lang.index_word)\n",
        "np.save('targ_lang_word_index.npy', targ_lang.word_index)\n",
        "np.save('targ_lang_index_word.npy', targ_lang.index_word)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC-lpAhRWNts"
      },
      "source": [
        "Loading checkpoint and vocabulary dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrn2QzEwij_F",
        "outputId": "cea26d6a-394c-4f21-cdf2-507e8c1bf1d4"
      },
      "source": [
        "# checkpoint.restore('./training_checkpoints/ckpt-5')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f700fc5cd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awSOcRJyWIO7"
      },
      "source": [
        "inp_lang_word_index = np.load('./inp_lang_word_index.npy', allow_pickle=True).all()\n",
        "inp_lang_index_word = np.load('./inp_lang_index_word.npy', allow_pickle=True).all()\n",
        "targ_lang_word_index = np.load('./targ_lang_word_index.npy', allow_pickle=True).all()\n",
        "targ_lang_index_word = np.load('./targ_lang_index_word.npy', allow_pickle=True).all()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBHN7GCLotpg"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang_word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang_word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang_index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang_index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5x1xTq2qfDT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "68d889a5-74b1-4ae8-d676-5b2aa35d214a"
      },
      "source": [
        "translate(u'I am going to the office')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> i am going to the office <end>\n",
            "Predicted translation: voy a la oficina . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGYGIcMGhUDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}